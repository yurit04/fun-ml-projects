{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction<a id=\"10\"></a>\n",
    "\n",
    "This competition requires us to predict future sales for Russian firm 1C Company. Our submission will contain next month sales predictions for 5100 items at each of 42 shops, with predictions being clipped into the range (0,20). The performance metric is RMSE. To inform these predictions we are given sales data covering the 33 months prior to the test period. For the (shop,item) pairs we are trying to predict, the following possibilities exist:\n",
    "1. This pair exists in our training set\n",
    "2. The item is in our training set, but not with the shop\n",
    "3. The item is not in our training set at all\n",
    "\n",
    "Our model needs to be capable of making predictions in all three cases.\n",
    "\n",
    "[1. Introduction](#10)  \n",
    "[2. Preparing Dataset](#20)  \n",
    "&nbsp;&nbsp;[2.1 Preparing Item/Category Information](#21)  \n",
    "&nbsp;&nbsp;[2.2 Preparing Sales Information](#22)  \n",
    "&nbsp;&nbsp;[2.3 Constructing Training Dataframe](#23)  \n",
    "&nbsp;&nbsp;[2.4 Adding Shop Information](#24)  \n",
    "&nbsp;&nbsp;[2.5 Ages & Aggregating Sales/Price information](#25)  \n",
    "&nbsp;&nbsp;[2.6 Lagging Values & Features that use Prior Information](#26)  \n",
    "&nbsp;&nbsp;[2.7 Encoding Name Information](#27)  \n",
    "[3. Modelling](#30)  \n",
    "&nbsp;&nbsp;[3.1 Training Model](#31)  \n",
    "&nbsp;&nbsp;[3.2 Submitting](#32)  \n",
    "[4. Analysing Model Output](#40)  \n",
    "&nbsp;&nbsp;[4.1 Plots](#41)  \n",
    "&nbsp;&nbsp;[4.2 Table Views](#42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "import category_encoders as ce\n",
    "import warnings\n",
    "\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', 160)\n",
    "pd.set_option('display.max_colwidth', 40)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing Training Dataset & Feature Engineering<a id=\"20\"></a>\n",
    "\n",
    "[1. Introduction](#10)  \n",
    "[2. Preparing Dataset](#20)  \n",
    "&nbsp;&nbsp;[2.1 Preparing Item/Category Information](#21)  \n",
    "&nbsp;&nbsp;[2.2 Preparing Sales Information](#22)  \n",
    "&nbsp;&nbsp;[2.3 Constructing Training Dataframe](#23)  \n",
    "&nbsp;&nbsp;[2.4 Adding Shop Information](#24)  \n",
    "&nbsp;&nbsp;[2.5 Ages & Aggregating Sales/Price information](#25)  \n",
    "&nbsp;&nbsp;[2.6 Lagging Values & Features that use Prior Information](#26)  \n",
    "&nbsp;&nbsp;[2.7 Encoding Name Information](#27)  \n",
    "[3. Modelling](#30)  \n",
    "&nbsp;&nbsp;[3.1 Training Model](#31)  \n",
    "&nbsp;&nbsp;[3.2 Submitting](#32)  \n",
    "[4. Analysing Model Output](#40)  \n",
    "&nbsp;&nbsp;[4.1 Plots](#41)  \n",
    "&nbsp;&nbsp;[4.2 Table Views](#42)  \n",
    "\n",
    "We'll be using shortened column names in this notebook to save some keystrokes and display more compactly. The submission file must name the target value 'item_cnt_month', but all sales data we work with will be monthly unless otherwise specified, so we will use 'item_cnt' until submission. The translated files already use category_id in place of item_category_id etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test set now as we'll soon be making use of the shop_id and item_id values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Preparing Item/Category Information** <a id=\"21\"></a>\n",
    "\n",
    "We load categories.csv and display the category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.read_csv('../input/predict-future-sales-eng-translation/categories.csv')\n",
    "pd.DataFrame(categories.category_name.values.reshape(-1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all category names have a '-' between the main category and the subcategory. We create groups by extracting the part of the name prior to a non-letter character. We then create a group_id column by label encoding the group names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create broader category groupings\n",
    "categories['group_name'] = categories['category_name'].str.extract(r'(^[\\w\\s]*)')\n",
    "categories['group_name'] = categories['group_name'].str.strip()\n",
    "#label encode group names\n",
    "categories['group_id']  = le.fit_transform(categories.group_name.values)\n",
    "categories.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load items.csv, clean the name column, create multiple features based on the cleaned name, and use label encoding. The categories dataframe is then joined to the items dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load items\n",
    "items = pd.read_csv('../input/predict-future-sales-eng-translation/items.csv')\n",
    "\n",
    "#clean item_name\n",
    "items['item_name'] = items['item_name'].str.lower()\n",
    "items['item_name'] = items['item_name'].str.replace('.', '')\n",
    "for i in [r'[^\\w\\d\\s\\.]', r'\\bthe\\b', r'\\bin\\b', r'\\bis\\b',\n",
    "          r'\\bfor\\b', r'\\bof\\b', r'\\bon\\b', r'\\band\\b',  \n",
    "          r'\\bto\\b', r'\\bwith\\b' , r'\\byo\\b']:\n",
    "    items['item_name'] = items['item_name'].str.replace(i, ' ')\n",
    "items['item_name'] = items['item_name'].str.replace(r'\\b.\\b', ' ')\n",
    "\n",
    "#extract first n characters of name\n",
    "items['item_name_no_space'] = items['item_name'].str.replace(' ', '')\n",
    "items['item_name_first4'] = [x[:4] for x in items['item_name_no_space']]\n",
    "items['item_name_first6'] = [x[:6] for x in items['item_name_no_space']]\n",
    "items['item_name_first11'] = [x[:11] for x in items['item_name_no_space']]\n",
    "del items['item_name_no_space']\n",
    "                              \n",
    "#label encode these columns\n",
    "items.item_name_first4 = le.fit_transform(items.item_name_first4.values)\n",
    "items.item_name_first6 = le.fit_transform(items.item_name_first6.values)\n",
    "items.item_name_first11 = le.fit_transform(items.item_name_first11.values)\n",
    "\n",
    "#join category_name, group_name and group_id to items\n",
    "items = items.join(categories.set_index('category_id'), on='category_id')\n",
    "items.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate rows exist in the item list. The following cell creates a dictionary that will allow us to reassign item id's where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupes = items[(items.duplicated(subset=['item_name','category_id'],keep=False))]\n",
    "dupes['in_test'] = dupes.item_id.isin(test.item_id.unique())\n",
    "dupes = dupes.groupby('item_name').agg({'item_id':['first','last'],'in_test':['first','last']})\n",
    "\n",
    "#if both item id's are in the test set do nothing\n",
    "dupes = dupes[(dupes[('in_test', 'first')]==False) | (dupes[('in_test', 'last')]==False)]\n",
    "#if only the first id is in the test set assign this id to both\n",
    "temp = dupes[dupes[('in_test', 'first')]==True]\n",
    "keep_first = dict(zip(temp[('item_id', 'last')], temp[('item_id',  'first')]))\n",
    "#if neither id or only the second id is in the test set, assign the second id to both\n",
    "temp = dupes[dupes[('in_test', 'first')]==False]\n",
    "keep_second = dict(zip(temp[('item_id', 'first')], temp[('item_id',  'last')]))\n",
    "item_map = {**keep_first, **keep_second}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Preparing Sales Information** <a id=\"22\"></a>\n",
    "\n",
    "We load sales.csv, remove the small proportion of rows without outlying values, use the dictionary we created above to reassign item id's where appropriate, then filter out sales for shops that don't exist in the test set and create features that need to be made before the data is grouped by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading sales data\n",
    "sales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\n",
    "sales = (sales\n",
    "    .query('0 < item_price < 50000 and 0 < item_cnt_day < 1001') #removing outliers\n",
    "    .replace({\n",
    "        'shop_id':{0:57, 1:58, 11:10}, #replacing obsolete shop id's\n",
    "        'item_id':item_map #fixing duplicate item id's  \n",
    "    })    \n",
    ")\n",
    "\n",
    "#removing shops which don't appear in the test set\n",
    "sales = sales[sales['shop_id'].isin(test.shop_id.unique())]\n",
    "\n",
    "sales['date'] = pd.to_datetime(sales.date,format='%d.%m.%Y')\n",
    "sales['weekday'] = sales.date.dt.dayofweek\n",
    "\n",
    "#first day the item was sold, day 0 is the first day of the training set period\n",
    "sales['first_sale_day'] = sales.date.dt.dayofyear \n",
    "sales['first_sale_day'] += 365 * (sales.date.dt.year-2013)\n",
    "sales['first_sale_day'] = sales.groupby('item_id')['first_sale_day'].transform('min').astype('int16')\n",
    "\n",
    "#revenue is needed to accurately calculate prices after grouping\n",
    "sales['revenue'] = sales['item_cnt_day']*sales['item_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the proportion of weekly sales that occurred on each weekday at each shop. Using this information we can assign a measure of weeks of sales power to each month. February always has 4 exactly weeks worth of days since there are no leap years in our time range and all other months have a value >4 since they have extra days of varying sales power. \n",
    "\n",
    "Month, year and first day of the month features are also created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = sales.groupby(['shop_id','weekday']).agg({'item_cnt_day':'sum'}).reset_index()\n",
    "temp = pd.merge(temp, sales.groupby(['shop_id']).agg({'item_cnt_day':'sum'}).reset_index(), on='shop_id', how='left')\n",
    "temp.columns = ['shop_id','weekday', 'shop_day_sales', 'shop_total_sales']\n",
    "temp['day_quality'] = temp['shop_day_sales']/temp['shop_total_sales']\n",
    "temp = temp[['shop_id','weekday','day_quality']]\n",
    "\n",
    "dates = pd.DataFrame(data={'date':pd.date_range(start='2013-01-01',end='2015-11-30')})\n",
    "dates['weekday'] = dates.date.dt.dayofweek\n",
    "dates['month'] = dates.date.dt.month\n",
    "dates['year'] = dates.date.dt.year - 2013\n",
    "dates['date_block_num'] = dates['year']*12 + dates['month'] - 1\n",
    "dates['first_day_of_month'] = dates.date.dt.dayofyear\n",
    "dates['first_day_of_month'] += 365 * dates['year']\n",
    "dates = dates.join(temp.set_index('weekday'), on='weekday')\n",
    "dates = dates.groupby(['date_block_num','shop_id','month','year']).agg({'day_quality':'sum','first_day_of_month':'min'}).reset_index()\n",
    "\n",
    "dates.query('shop_id == 28').head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now group the sales data by month, shop_id a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = (sales\n",
    "     .groupby(['date_block_num', 'shop_id', 'item_id'])\n",
    "     .agg({\n",
    "         'item_cnt_day':'sum', \n",
    "         'revenue':'sum',\n",
    "         'first_sale_day':'first'\n",
    "     })\n",
    "     .reset_index()\n",
    "     .rename(columns={'item_cnt_day':'item_cnt'})\n",
    ")\n",
    "sales.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Constructing Training Dataframe** <a id=\"23\"></a>\n",
    "\n",
    "The test set consists of the cartesian product of 42 shops and 5100 items. To make a training set which approximates the test set we create a training dataframe consisting of the cartesian product (active items) x (active shops) for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    df.append(np.array(list(product(*[cur_shops, cur_items, [block_num]]))))\n",
    "\n",
    "df = pd.DataFrame(np.vstack(df), columns=['shop_id', 'item_id', 'date_block_num'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the appropriate date_block_num value to the test set\n",
    "test['date_block_num'] = 34\n",
    "del test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append test set to training dataframe\n",
    "df = pd.concat([df,test]).fillna(0)\n",
    "df = df.reset_index()\n",
    "del df['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join sales and item inforamtion to the training dataframe\n",
    "df = pd.merge(df, sales, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n",
    "df = pd.merge(df, dates, on=['date_block_num','shop_id'], how='left')\n",
    "df = pd.merge(df, items.drop(columns=['item_name','group_name','category_name']), on='item_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Adding Shop Information** <a id=\"24\"></a>\n",
    "\n",
    "We cluster the shops using K-means clustering, using the proportion of their sales they make in each category as features.\n",
    "\n",
    "k=7 was selected because:\n",
    "* k=7 resulted in the highest average silhouette score aside from a choice of k=2. \n",
    "* k=2 would not provide a useful clustering because it creates a feature with value (shop_id==55)\\*1. \n",
    "* k=7 is also in an appropriate area when using the elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading shops.csv\n",
    "shops = pd.read_csv('../input/predict-future-sales-eng-translation/shops.csv')\n",
    "\n",
    "#clustering shops\n",
    "shops_cats = pd.DataFrame(\n",
    "    np.array(list(product(*[df['shop_id'].unique(), df['category_id'].unique()]))),\n",
    "    columns =['shop_id', 'category_id']\n",
    ")\n",
    "temp = df.groupby(['category_id', 'shop_id']).agg({'item_cnt':'sum'}).reset_index()\n",
    "temp2 = temp.groupby('shop_id').agg({'item_cnt':'sum'}).rename(columns={'item_cnt':'shop_total'})\n",
    "temp = temp.join(temp2, on='shop_id')\n",
    "temp['category_proportion'] = temp['item_cnt']/temp['shop_total']\n",
    "temp = temp[['shop_id', 'category_id', 'category_proportion']]\n",
    "shops_cats = pd.merge(shops_cats, temp, on=['shop_id','category_id'], how='left')\n",
    "shops_cats = shops_cats.fillna(0)\n",
    "\n",
    "shops_cats = shops_cats.pivot(index='shop_id', columns=['category_id'])\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(shops_cats)\n",
    "shops_cats['shop_cluster'] = kmeans.labels_.astype('int8')\n",
    "\n",
    "#adding these clusters to the shops dataframe\n",
    "shops = shops.join(shops_cats['shop_cluster'], on='shop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the shop names column then use the opening word to create the shop_city feature. We then create the shop_type feature based on terms that occur in the name of the shop. Both these features are then label encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unused shop ids\n",
    "shops.dropna(inplace=True)\n",
    "\n",
    "#cleaning the name column\n",
    "shops['shop_name'] = shops['shop_name'].str.lower()\n",
    "shops['shop_name'] = shops['shop_name'].str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "#creating a column for the type of shop\n",
    "shops['shop_type'] = 'regular'\n",
    "\n",
    "#there is some overlap in tc and mall, mall is given precedence\n",
    "shops.loc[shops['shop_name'].str.contains(r'tc'), 'shop_type'] = 'tc'\n",
    "shops.loc[shops['shop_name'].str.contains(r'mall|center|mega'), 'shop_type'] = 'mall'\n",
    "shops.loc[shops['shop_id'].isin([9,20]), 'shop_type'] = 'special'\n",
    "shops.loc[shops['shop_id'].isin([12,55]), 'shop_type'] = 'online'\n",
    "\n",
    "#the first word of shop name is largely sufficient as a city feature\n",
    "shops['shop_city'] = shops['shop_name'].str.split().str[0]\n",
    "shops.loc[shops['shop_id'].isin([12,55]), 'shop_city'] = 'online'\n",
    "shops.shop_city = le.fit_transform(shops.shop_city.values)\n",
    "shops.shop_type = le.fit_transform(shops.shop_type.values)\n",
    "shops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add shop information to the training dataframe\n",
    "df = pd.merge(df, shops.drop(columns='shop_name'), on='shop_id', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Ages & Aggregating Sales/Price information** <a id=\"25\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a feature showing how many days have passed between the first time an item was sold and the beginning of the current month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_sale_day'] = df.groupby('item_id')['first_sale_day'].transform('max').astype('int16')\n",
    "df.loc[df['first_sale_day']==0, 'first_sale_day'] = 1035\n",
    "df['prev_days_on_sale'] = [max(idx) for idx in zip(df['first_day_of_month']-df['first_sale_day'],[0]*len(df))]\n",
    "del df['first_day_of_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeing RAM, removing unneeded columns and encoding object columns\n",
    "del sales, categories, shops, shops_cats, temp, temp2, test, dupes, item_map, \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to clip the target value before aggregating so that mean values are not distorted due to outliers. We retain the unclipped value for use in features that do not aggregate the sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_cnt_unclipped'] = df['item_cnt']\n",
    "df['item_cnt'] = df['item_cnt'].clip(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No columns with float dtype require more than float32 precision and no int dtype columns require values outside the int16 range. The following function will compress the data types of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df):\n",
    "    #reduce size of the dataframe\n",
    "    float_cols = [c for c in df if df[c].dtype in [\"float64\"]]\n",
    "    int_cols = [c for c in df if df[c].dtype in ['int64']]\n",
    "    df[float_cols] = df[float_cols].astype('float32')\n",
    "    df[int_cols] = df[int_cols].astype('int16')\n",
    "    return df\n",
    "df = downcast(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features show how many months have passed since the first appearance of the item/name/category/group/shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_age'] = (df['date_block_num'] - df.groupby('item_id')['date_block_num'].transform('min')).astype('int8')\n",
    "df['item_name_first4_age'] = (df['date_block_num'] - df.groupby('item_name_first4')['date_block_num'].transform('min')).astype('int8')\n",
    "df['item_name_first6_age'] = (df['date_block_num'] - df.groupby('item_name_first6')['date_block_num'].transform('min')).astype('int8')\n",
    "df['item_name_first11_age'] = (df['date_block_num'] - df.groupby('item_name_first11')['date_block_num'].transform('min')).astype('int8')\n",
    "df['category_age'] = (df['date_block_num'] - df.groupby('category_id')['date_block_num'].transform('min')).astype('int8')\n",
    "df['group_age'] = (df['date_block_num'] - df.groupby('group_id')['date_block_num'].transform('min')).astype('int8')\n",
    "df['shop_age'] = (df['date_block_num'] - df.groupby('shop_id')['date_block_num'].transform('min')).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicates whether shops have previously sold the item\n",
    "temp = df.query('item_cnt > 0').groupby(['item_id','shop_id']).agg({'date_block_num':'min'}).reset_index()\n",
    "temp.columns = ['item_id', 'shop_id', 'item_shop_first_sale']\n",
    "df = pd.merge(df, temp, on=['item_id','shop_id'], how='left')\n",
    "df['item_shop_first_sale'] = df['item_shop_first_sale'].fillna(50)\n",
    "#item age that stays at 0 if a shop hasn't sold the item\n",
    "df['item_age_if_shop_sale'] = (df['date_block_num'] > df['item_shop_first_sale']) * df['item_age']\n",
    "#the length of time an item has been for sale without being sold at individual shops\n",
    "df['item_age_without_shop_sale'] = (df['date_block_num'] <= df['item_shop_first_sale']) * df['item_age']\n",
    "del df['item_shop_first_sale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable, 'item_cnt', is the monthly sale count of individual items at individual shops. We now create features showing average monthly sales based on various groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_cnt_col(df, merging_cols, new_col,aggregation):\n",
    "    temp = df.groupby(merging_cols).agg(aggregation).reset_index()\n",
    "    temp.columns = merging_cols + [new_col]\n",
    "    df = pd.merge(df, temp, on=merging_cols, how='left')\n",
    "    return df\n",
    "\n",
    "#individual items across all shops\n",
    "df = agg_cnt_col(df, ['date_block_num','item_id'],'item_cnt_all_shops',{'item_cnt':'mean'})\n",
    "df = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'item_cnt_all_shops_median',{'item_cnt':'median'}) \n",
    "#all items in category at individual shops\n",
    "df = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'category_cnt',{'item_cnt':'mean'})\n",
    "df = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'category_cnt_median',{'item_cnt':'median'}) \n",
    "#all items in category across all shops\n",
    "df = agg_cnt_col(df, ['date_block_num','category_id'],'category_cnt_all_shops',{'item_cnt':'mean'})\n",
    "df = agg_cnt_col(df, ['date_block_num','category_id'],'category_cnt_all_shops_median',{'item_cnt':'median'})\n",
    "#all items in group\n",
    "df = agg_cnt_col(df, ['date_block_num','group_id','shop_id'],'group_cnt',{'item_cnt':'mean'})\n",
    "#all items in group across all shops\n",
    "df = agg_cnt_col(df, ['date_block_num','group_id'],'group_cnt_all_shops',{'item_cnt':'mean'})\n",
    "#all items at individual shops\n",
    "df = agg_cnt_col(df, ['date_block_num','shop_id'],'shop_cnt',{'item_cnt':'mean'})\n",
    "#all items at all shops within the city\n",
    "df = agg_cnt_col(df, ['date_block_num','shop_city'],'city_cnt',{'item_cnt':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create features showing the mean first month sales for items in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_item_sales(df, merging_cols, new_col):\n",
    "    temp = (\n",
    "        df\n",
    "        .query('item_age==0')\n",
    "        .groupby(merging_cols)['item_cnt']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'item_cnt': new_col})\n",
    "    )\n",
    "    df = pd.merge(df, temp, on=merging_cols, how='left')\n",
    "    return df\n",
    "\n",
    "#mean units sold of new item in category at individual shop\n",
    "df = new_item_sales(df, ['date_block_num','category_id','shop_id'], 'new_items_in_cat')\n",
    "#mean units sold of new item in category across all shops\n",
    "df = new_item_sales(df, ['date_block_num','category_id'], 'new_items_in_cat_all_shops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_price_col(df, merging_cols, new_col):\n",
    "    temp = df.groupby(merging_cols).agg({'revenue':'sum','item_cnt_unclipped':'sum'}).reset_index()\n",
    "    temp[new_col] = temp['revenue']/temp['item_cnt_unclipped']\n",
    "    temp = temp[merging_cols + [new_col]]\n",
    "    df = pd.merge(df, temp, on=merging_cols, how='left')\n",
    "    return df\n",
    "\n",
    "#average item price\n",
    "df = agg_price_col(df,['date_block_num','item_id'],'item_price')\n",
    "#average price of items in category\n",
    "df = agg_price_col(df,['date_block_num','category_id'],'category_price')\n",
    "#average price of all items\n",
    "df = agg_price_col(df,['date_block_num'],'block_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = downcast(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 Lagging Values & Features that use Prior Information** <a id=\"26\"></a>\n",
    "\n",
    "The following function will be used to create lag features of varying lag periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_feature(df, lag, col, merge_cols):        \n",
    "    temp = df[merge_cols + [col]]\n",
    "    temp = temp.groupby(merge_cols).agg({f'{col}':'first'}).reset_index()\n",
    "    temp.columns = merge_cols + [f'{col}_lag{lag}']\n",
    "    temp['date_block_num'] += lag\n",
    "    df = pd.merge(df, temp, on=merge_cols, how='left')\n",
    "    df[f'{col}_lag{lag}'] = df[f'{col}_lag{lag}'].fillna(0).astype('float32')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features each have 3 lagged columns returned:\n",
    "* lag1 shows the value of the prior month\n",
    "* lag2 shows the value two months prior\n",
    "* lag1to12 is the sum of values across the previous 12 months\n",
    "\n",
    "Original columns will be deleted when they are no longer needed to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag12_cols = {\n",
    "    'item_cnt':['date_block_num', 'shop_id', 'item_id'],\n",
    "    'item_cnt_all_shops':['date_block_num', 'item_id'],\n",
    "    'category_cnt':['date_block_num', 'shop_id', 'category_id'],\n",
    "    'category_cnt_all_shops':['date_block_num', 'category_id'],\n",
    "    'group_cnt':['date_block_num', 'shop_id', 'group_id'],\n",
    "    'group_cnt_all_shops':['date_block_num', 'group_id'],\n",
    "    'shop_cnt':['date_block_num', 'shop_id'],\n",
    "    'city_cnt':['date_block_num', 'shop_city'],\n",
    "    'new_items_in_cat':['date_block_num', 'shop_id', 'category_id'],\n",
    "    'new_items_in_cat_all_shops':['date_block_num', 'category_id']\n",
    "}\n",
    "for col,merge_cols in lag12_cols.items():\n",
    "    df[f'{col}_lag1to12'] = 0\n",
    "    for i in range(1,13):\n",
    "        df = lag_feature(df, i, col, merge_cols)\n",
    "        df[f'{col}_lag1to12'] += df[f'{col}_lag{i}']\n",
    "        if i > 2:\n",
    "            del df[f'{col}_lag{i}']\n",
    "    if col == 'item_cnt':\n",
    "        del df[f'{col}_lag1']\n",
    "        del df[f'{col}_lag2']        \n",
    "    else:\n",
    "        del df[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take lag1 and lag2 values for these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag2_cols = {\n",
    "    'item_cnt_unclipped':['date_block_num', 'shop_id', 'item_id'],\n",
    "    'item_cnt_all_shops_median':['date_block_num', 'item_id'],\n",
    "    'category_cnt_median':['date_block_num', 'shop_id', 'category_id'],\n",
    "    'category_cnt_all_shops_median':['date_block_num', 'category_id']\n",
    "}\n",
    "for col in lag2_cols:\n",
    "    df = lag_feature(df, 1, col, merge_cols)\n",
    "    df = lag_feature(df, 2, col, merge_cols)\n",
    "    if col!='item_cnt_unclipped':\n",
    "        del df[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features show the ratio between lag1 and lag1to12 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_cnt_diff'] = df['item_cnt_unclipped_lag1']/df['item_cnt_lag1to12']\n",
    "df['item_cnt_all_shops_diff'] = df['item_cnt_all_shops_lag1']/df['item_cnt_all_shops_lag1to12']\n",
    "df['category_cnt_diff'] = df['category_cnt_lag1']/df['category_cnt_lag1to12']\n",
    "df['category_cnt_all_shops_diff'] = df['category_cnt_all_shops_lag1']/df['category_cnt_all_shops_lag1to12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create lag1 values for category and block prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lag_feature(df, 1, 'category_price',['date_block_num', 'category_id'])\n",
    "df = lag_feature(df, 1, 'block_price',['date_block_num'])\n",
    "del df['category_price'], df['block_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill the missing values in the below columns with 0 because we know this is the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['item_age']>0) & (df['item_cnt_lag1to12'].isna()), 'item_cnt_lag1to12'] = 0\n",
    "df.loc[(df['category_age']>0) & (df['category_cnt_lag1to12'].isna()), 'category_cnt_lag1to12'] = 0\n",
    "df.loc[(df['group_age']>0) & (df['group_cnt_lag1to12'].isna()), 'group_cnt_lag1to12'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, lag1to12 values are the sum of values over the previous 12 months. Differing ages in the dataset mean that some lag1to12 values are calculated over the previous 12 months, but others have had less time to accrue. \n",
    "\n",
    "We divide lag1to12 values by the minimum between 12 and previous periods in the dataset. This turns lag1to12 into a monthly average that can be more accurately compared between datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_cnt_lag1to12'] /= [min(idx) for idx in zip(df['item_age'],df['shop_age'],[12]*len(df))]\n",
    "df['item_cnt_all_shops_lag1to12'] /= [min(idx) for idx in zip(df['item_age'],[12]*len(df))]\n",
    "df['category_cnt_lag1to12'] /= [min(idx) for idx in zip(df['category_age'],df['shop_age'],[12]*len(df))]\n",
    "df['category_cnt_all_shops_lag1to12'] /= [min(idx) for idx in zip(df['category_age'],[12]*len(df))]\n",
    "df['group_cnt_lag1to12'] /= [min(idx) for idx in zip(df['group_age'],df['shop_age'],[12]*len(df))]\n",
    "df['group_cnt_all_shops_lag1to12'] /= [min(idx) for idx in zip(df['group_age'],[12]*len(df))]\n",
    "df['city_cnt_lag1to12'] /= [min(idx) for idx in zip(df['date_block_num'],[12]*len(df))]\n",
    "df['shop_cnt_lag1to12'] /= [min(idx) for idx in zip(df['shop_age'],[12]*len(df))]\n",
    "df['new_items_in_cat_lag1to12'] /= [min(idx) for idx in zip(df['category_age'],df['shop_age'],[12]*len(df))]\n",
    "df['new_items_in_cat_all_shops_lag1to12'] /= [min(idx) for idx in zip(df['category_age'],[12]*len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = downcast(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features use past information across more than one period of the dataset. They are not suitable to be lagged in the normal manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def past_information(df, merging_cols, new_col, aggregation):\n",
    "    temp = []\n",
    "    for i in range(1,35):\n",
    "        block = df.query(f'date_block_num < {i}').groupby(merging_cols).agg(aggregation).reset_index()\n",
    "        block.columns = merging_cols + [new_col]\n",
    "        block['date_block_num'] = i\n",
    "        block = block[block[new_col]>0]\n",
    "        temp.append(block)\n",
    "    temp = pd.concat(temp)\n",
    "    df = pd.merge(df, temp, on=['date_block_num']+merging_cols, how='left')\n",
    "    return df\n",
    "\n",
    "#average item price in latest block item was sold\n",
    "df = past_information(df, ['item_id'],'last_item_price',{'item_price':'last'})\n",
    "#total units of item sold at individual shop\n",
    "df = past_information(df, ['shop_id','item_id'],'item_cnt_sum_alltime',{'item_cnt':'sum'})\n",
    "#total units of item sold at all shops\n",
    "df = past_information(df, ['item_id'],'item_cnt_sum_alltime_allshops',{'item_cnt':'sum'})\n",
    "\n",
    "#these columns are no longer needed, and would cause data leakage if retained\n",
    "del df['revenue'], df['item_cnt_unclipped'], df['item_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature showing item prices relative to the block price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['relative_price_item_block_lag1'] = df['last_item_price']/df['block_price_lag1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features show per day sales values since an item was first sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['item_cnt_per_day_alltime'] = (df['item_cnt_sum_alltime']/df['prev_days_on_sale']).fillna(0)\n",
    "df['item_cnt_per_day_alltime_allshops'] = (df['item_cnt_sum_alltime_allshops']/df['prev_days_on_sale']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "df = downcast(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features show the mean sales of items with names that have the same first n characters, in the same category and at the same item age. The hope is that this feature can catch past performance of similar items such as earlier titles in a series, particularly in their debut month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_name_cat_age(df,n,all_shops):\n",
    "    temp_cols = [f'same_name{n}catage_cnt','date_block_num', f'item_name_first{n}','item_age','category_id']\n",
    "    if all_shops:\n",
    "        temp_cols[0] += '_all_shops'\n",
    "    else:\n",
    "        temp_cols += ['shop_id']\n",
    "    temp = []\n",
    "    for i in range(1,35):\n",
    "        block = (\n",
    "            df\n",
    "            .query(f'date_block_num < {i}')\n",
    "            .groupby(temp_cols[2:])\n",
    "            .agg({'item_cnt':'mean'})\n",
    "            .reset_index()\n",
    "            .rename(columns={'item_cnt':temp_cols[0]})\n",
    "        )\n",
    "        block = block[block[temp_cols[0]]>0]\n",
    "        block['date_block_num'] = i\n",
    "        temp.append(block)\n",
    "    temp = pd.concat(temp)\n",
    "    df = pd.merge(df, temp, on=temp_cols[1:], how='left')\n",
    "    return df\n",
    "\n",
    "for n in [4,6,11]:\n",
    "    for all_shops in [True,False]:\n",
    "        df = matching_name_cat_age(df,n,all_shops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign appropriate datatypes\n",
    "df = downcast(df)\n",
    "int8_cols = [\n",
    "    'item_cnt','month','group_id','shop_type',\n",
    "    'shop_city','shop_id','date_block_num','category_id',\n",
    "    'item_age',\n",
    "]\n",
    "int16_cols = [\n",
    "    'item_id','item_name_first4',\n",
    "    'item_name_first6','item_name_first11'\n",
    "]\n",
    "for col in int8_cols:\n",
    "    df[col] = df[col].astype('int8')\n",
    "for col in int16_cols:\n",
    "    df[col] = df[col].astype('int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features show sales data for items with item_id 1 above and 1 below the item in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearby_item_data(df,col):\n",
    "    if col in ['item_cnt_unclipped_lag1','item_cnt_lag1to12']:\n",
    "        cols = ['date_block_num', 'shop_id', 'item_id']\n",
    "        temp = df[cols + [col]] \n",
    "    else:\n",
    "        cols = ['date_block_num', 'item_id']\n",
    "        temp = df.groupby(cols).agg({col:'first'}).reset_index()[cols + [col]]   \n",
    "    \n",
    "    temp.columns = cols + [f'below_{col}']\n",
    "    temp['item_id'] += 1\n",
    "    df = pd.merge(df, temp, on=cols, how='left')\n",
    "    \n",
    "    temp.columns = cols + [f'above_{col}']\n",
    "    temp['item_id'] -= 2\n",
    "    df = pd.merge(df, temp, on=cols, how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "item_cols = ['item_cnt_unclipped_lag1','item_cnt_lag1to12',\n",
    "             'item_cnt_all_shops_lag1','item_cnt_all_shops_lag1to12']\n",
    "for col in item_cols:\n",
    "    df = nearby_item_data(df,col)\n",
    "    \n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 Encoding Name Information** <a id=\"27\"></a>\n",
    "\n",
    "We add boolean features indicating whether an item's name contains any words which frequently appear in the item set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Counter()\n",
    "items['item_name'].str.split().apply(results.update)\n",
    "\n",
    "words = []\n",
    "cnts = []\n",
    "for key, value in results.items():\n",
    "    words.append(key)\n",
    "    cnts.append(value)\n",
    "    \n",
    "counts = pd.DataFrame({'word':words,'count':cnts})\n",
    "common_words = counts.query('count>200').word.to_list()\n",
    "for word in common_words:\n",
    "    items[f'{word}_in_name'] = items['item_name'].str.contains(word).astype('int8')\n",
    "drop_cols = [\n",
    "    'item_id','category_id','item_name','item_name_first4',\n",
    "    'item_name_first6','item_name_first11',\n",
    "    'category_name','group_name','group_id'\n",
    "]\n",
    "items = items.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join these word vectors to the training dataframe\n",
    "df = df.join(items, on='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given it's high cardinality, we use binary encoding to create a better representation of item_name_first11. The other name features are now deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encode(df, letters, cols):\n",
    "    encoder = ce.BinaryEncoder(cols=[f'item_name_first{letters}'], return_df=True)\n",
    "    temp = encoder.fit_transform(df[f'item_name_first{letters}'])\n",
    "    df = pd.concat([df,temp], axis=1)\n",
    "    del df[f'item_name_first{letters}_0']\n",
    "    name_cols = [f'item_name_first{letters}_{x}' for x in range(1,cols)]\n",
    "    df[name_cols] = df[name_cols].astype('int8')\n",
    "    return df\n",
    "\n",
    "df = binary_encode(df, 11, 15)\n",
    "    \n",
    "del df['item_name_first4'], df['item_name_first6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe for later use\n",
    "df.to_pickle('df_complete.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the kernel to clear memory.\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling<a id=\"30\"></a>\n",
    "[1. Introduction](#10)  \n",
    "[2. Preparing Dataset](#20)  \n",
    "&nbsp;&nbsp;[2.1 Preparing Item/Category Information](#21)  \n",
    "&nbsp;&nbsp;[2.2 Preparing Sales Information](#22)  \n",
    "&nbsp;&nbsp;[2.3 Constructing Training Dataframe](#23)  \n",
    "&nbsp;&nbsp;[2.4 Adding Shop Information](#24)  \n",
    "&nbsp;&nbsp;[2.5 Ages & Aggregating Sales/Price information](#25)  \n",
    "&nbsp;&nbsp;[2.6 Lagging Values & Features that use Prior Information](#26)  \n",
    "&nbsp;&nbsp;[2.7 Encoding Name Information](#27)  \n",
    "[3. Modelling](#30)  \n",
    "&nbsp;&nbsp;[3.1 Training Model](#31)  \n",
    "&nbsp;&nbsp;[3.2 Submitting](#32)  \n",
    "[4. Analysing Model Output](#40)  \n",
    "&nbsp;&nbsp;[4.1 Plots](#41)  \n",
    "&nbsp;&nbsp;[4.2 Table Views](#42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pd.set_option('display.max_rows', 160)\n",
    "pd.set_option('display.max_columns', 160)\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the training, validation, and test sets. The first 2 months are not used for training as many feature values are likely to be misrepresentative in this period. \n",
    "\n",
    "The month of data directly before the test period is used as our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved training dataframe\n",
    "df = pd.read_pickle('../input/files-top-scoring-notebook-output-exploration/df_complete.pkl')\n",
    "\n",
    "X_train = df[~df.date_block_num.isin([0,1,33,34])]\n",
    "y_train = X_train['item_cnt']\n",
    "del X_train['item_cnt']\n",
    "\n",
    "X_val = df[df['date_block_num']==33]\n",
    "y_val = X_val['item_cnt']\n",
    "del X_val['item_cnt']\n",
    "\n",
    "X_test = df[df['date_block_num']==34].drop(columns='item_cnt')\n",
    "X_test = X_test.reset_index()\n",
    "del X_test['index']\n",
    "\n",
    "#free memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Training Model** <a id=\"31\"></a>\n",
    "\n",
    "Hyperparameter values were selected using a gridsearch with ranges of values for num_leaves, feature_fraction, bagging_fraction and min_data_in_leaf. \n",
    "\n",
    "The combination of values which resulted in the lowest validation set RMSE was selected. \n",
    "\n",
    "Learning rate is set to 0.01 as this value results in a well-behaved learning curve. Default values for other hyperparameters were deemed appropriate.\n",
    "\n",
    "The moderately sized gap between training set RMSE and validation set RMSE implies some overfitting may be occurring, however, no tweaking of hyperparameters to constrict model flexibility resulted in a lower validation RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_val = lgb.Dataset(X_val, y_val)\n",
    "    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=50,\n",
    "                     categorical_feature=cat_features)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip this cell if directly loading saved model \n",
    "params = {\n",
    "    'objective': 'rmse',\n",
    "    'metric': 'rmse',\n",
    "    'num_leaves': 1023,\n",
    "    'min_data_in_leaf':10,\n",
    "    'feature_fraction':0.7,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_rounds': 1000,\n",
    "    'early_stopping_rounds': 30,\n",
    "    'seed': 1\n",
    "}\n",
    "#designating the categorical features which should be focused on\n",
    "cat_features = ['category_id','month','shop_id','shop_city']\n",
    "\n",
    "lgb_model = build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features)\n",
    "\n",
    "#save model for later use\n",
    "lgb_model.save_model('initial_lgb_model.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you wish to explore the model without retraining, it can be directly loaded by uncommenting and running the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb_model = lgb.Booster(model_file='../input/files-top-scoring-notebook-output-exploration/initial_lgb_model.txt')#\n",
    "#lgb_model.params['objective'] = 'rmse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Submitting** <a id=\"32\"></a>\n",
    "\n",
    "We use this model to predict on the test set, and clip the predictions into the range (0,20) before submitting. \n",
    "\n",
    "This achieves a score of 0.85389 on the public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')\n",
    "submission['item_cnt_month'] = lgb_model.predict(X_test).clip(0,20)\n",
    "submission[['ID', 'item_cnt_month']].to_csv('initial_lgb_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysing Model Output<a id=\"40\"></a>\n",
    "[1. Introduction](#10)  \n",
    "[2. Preparing Dataset](#20)  \n",
    "&nbsp;&nbsp;[2.1 Preparing Item/Category Information](#21)  \n",
    "&nbsp;&nbsp;[2.2 Preparing Sales Information](#22)  \n",
    "&nbsp;&nbsp;[2.3 Constructing Training Dataframe](#23)  \n",
    "&nbsp;&nbsp;[2.4 Adding Shop Information](#24)  \n",
    "&nbsp;&nbsp;[2.5 Ages & Aggregating Sales/Price information](#25)  \n",
    "&nbsp;&nbsp;[2.6 Lagging Values & Features that use Prior Information](#26)  \n",
    "&nbsp;&nbsp;[2.7 Encoding Name Information](#27)  \n",
    "[3. Modelling](#30)  \n",
    "&nbsp;&nbsp;[3.1 Training Model](#31)  \n",
    "&nbsp;&nbsp;[3.2 Submitting](#32)  \n",
    "[4. Analysing Model Output](#40)  \n",
    "&nbsp;&nbsp;[4.1 Plots](#41)  \n",
    "&nbsp;&nbsp;[4.2 Table Views](#42)  \n",
    "\n",
    "By looking more closely at the test set and our model output, we can make adjustments to improve on our initial model. The exploration shown below can provide insights that will allow us to make beneficial changes to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#load item/category/shop information to make analysing the results easier\n",
    "categories = pd.read_csv('../input/predict-future-sales-eng-translation/categories.csv')\n",
    "categories['group_name'] = categories['category_name'].str.extract(r'(^[\\w\\s]*)')\n",
    "categories['group_name'] = categories['group_name'].str.strip()\n",
    "\n",
    "items = pd.read_csv('../input/predict-future-sales-eng-translation/items.csv')\n",
    "items['item_name'] = items['item_name'].str.lower()\n",
    "for i in [r'[^\\w\\d\\s]', r'\\bthe\\b', r'\\bin\\b', r'\\bfor\\b', r'\\bof\\b', r'\\bd\\b', r'\\bis\\b', r'\\bon\\b']:\n",
    "    items['item_name'] = items['item_name'].str.replace(i, ' ')\n",
    "items['item_name'] = items['item_name'].str.replace(' ', '')\n",
    "items = items.join(categories.set_index('category_id'), on='category_id')\n",
    "\n",
    "shops = pd.read_csv('../input/predict-future-sales-eng-translation/shops.csv')\n",
    "shops['shop_name'] = shops['shop_name'].str.lower()\n",
    "shops['shop_name'] = shops['shop_name'].str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "shops['shop_city'] = shops['shop_name'].str.split().str[0]\n",
    "shops.loc[shops['shop_id'].isin([12,55]), 'shop_city'] = 'online'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add columns showing predicted values, along with target values and sq_error for our training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['lgb_pred'] = lgb_model.predict(X_train).clip(0,20)\n",
    "X_train['target'] = y_train\n",
    "X_train['sq_err'] = (X_train['lgb_pred']-X_train['target'])**2\n",
    "\n",
    "X_val['lgb_pred'] = lgb_model.predict(X_val).clip(0,20)\n",
    "X_val['target'] = y_val\n",
    "X_val['sq_err'] = (X_val['lgb_pred']-X_val['target'])**2\n",
    "\n",
    "X_test['lgb_pred'] = lgb_model.predict(X_test).clip(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataframe that will allow us to easily graph some aspects of model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_train.groupby('date_block_num').agg({'lgb_pred':'mean','target':'mean','sq_err':'mean'}).reset_index()\n",
    "data['new_item_rmse'] = np.sqrt(X_train.query('item_age<=1').groupby('date_block_num').agg({'sq_err':'mean'}).sq_err)\n",
    "data['old_item_rmse'] = np.sqrt(X_train.query('item_age>1').groupby('date_block_num').agg({'sq_err':'mean'}).sq_err)\n",
    "data = data.append([\n",
    "    {'date_block_num':33,\n",
    "     'target':X_val.target.mean(),\n",
    "     'lgb_pred':X_val.lgb_pred.mean(),\n",
    "     'sq_err':np.sqrt(X_val.sq_err.mean()),\n",
    "     'old_item_rmse':np.sqrt(X_val.query('item_age>1').sq_err.mean()),\n",
    "     'new_item_rmse':np.sqrt(X_val.query('item_age<=1').sq_err.mean())},\n",
    "    {'date_block_num':34,\n",
    "     'target':0,\n",
    "     'lgb_pred':X_test.lgb_pred.mean(),\n",
    "     'sq_err':0,\n",
    "     'old_item_rmse':0,\n",
    "     'new_item_rmse':0}\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "data['date'] = [x[:7] for x in pd.date_range(start='2013-03',end='2015-09',freq='MS').astype('str')]+['Validation','Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Plots** <a id=\"41\"></a>\n",
    "\n",
    "A summary plot of feature importance. We can use this along with other [shap](https://shap.readthedocs.io/en/latest/) plots to get an idea of what drives the models predictions, and help us find spurious features which can be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = X_test.drop(columns='lgb_pred').sample(10000)\n",
    "explainer = shap.TreeExplainer(lgb_model)\n",
    "shap_values = explainer.shap_values(temp)\n",
    "shap.summary_plot(shap_values, temp, max_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Prediction',x=data.date,y=data.lgb_pred),\n",
    "    go.Bar(name='Target',x=data.date,y=data.target)\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Mean Prediction and Target Values by Month',\n",
    "    xaxis={'title':'Month','type':'category'},\n",
    "    yaxis={'title':'Mean Value'},\n",
    "    legend={'yanchor':'top','y':1.05,'xanchor':'left','x':0.01},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "text = '''\n",
    "    The model may not be adequately accounting for<br>\n",
    "    the December sales spike. There is no systemic under or<br>\n",
    "    over-prediction visible for the test month, November.\n",
    "'''\n",
    "fig.add_annotation(\n",
    "    yref='paper', y=1.1,\n",
    "    xref='paper', x=0.7,\n",
    "    text=text,\n",
    "    font={'size':11},\n",
    "    showarrow=False)\n",
    "\n",
    "text = '''\n",
    "    Prediction mean is very close to the  <br> \n",
    "    target mean in our validation set.\n",
    "'''\n",
    "fig.add_annotation(\n",
    "    xref='paper', x=0.95,\n",
    "    yref='paper', y=0.58,\n",
    "    text=text,\n",
    "    font={'size':11},\n",
    "    showarrow=True, arrowhead=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Bar(\n",
    "    x=data.date[:-1],\n",
    "    y=data.sq_err[:-1],\n",
    "    marker_color=[x%12 for x in range(2,34)]\n",
    ")])\n",
    "fig.update_layout(\n",
    "    title='RMSE by Month',\n",
    "    xaxis={'title':'Month','type':'category'},\n",
    "    yaxis={'title':'RMSE'},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "text = '''\n",
    "    In the training set, RMSE is highest in December. <br>\n",
    "    \n",
    "'''\n",
    "fig.add_annotation(\n",
    "    yref='paper', y=0.9,\n",
    "    xref='paper', x=0.1,\n",
    "    text=text,\n",
    "    font={'size':11},\n",
    "    showarrow=False)\n",
    "text = '''\n",
    "    Validation RMSE is higher than in any month of <br>\n",
    "    the training set. This is expected given the <br>\n",
    "    gap in training set and validation set RMSE we <br>\n",
    "    saw when training the model.\n",
    "'''\n",
    "fig.add_annotation(\n",
    "    yref='paper', y=1.1,\n",
    "    xref='paper', x=1,\n",
    "    text=text,\n",
    "    font={'size':11},\n",
    "    showarrow=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "temp = X_val.groupby('item_age').agg({'sq_err':'mean','target':'mean'}).reset_index()\n",
    "temp['sq_err'] = np.sqrt(temp['sq_err'])\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='RMSE',x=temp.item_age,y=temp.sq_err)\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='Validation set RMSE by Item Age',\n",
    "    xaxis={'title':'Item Age','type':'category'},\n",
    "    yaxis={'title':'RMSE'},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "text = '''\n",
    "    RMSE is much higher for items with age 0 or 1. There is a <br>\n",
    "    sharp decrease in RMSE between ages 0 and 2, then a much <br>\n",
    "    flatter downwards trend as age increases beyond 2.\n",
    "'''\n",
    "fig.add_annotation(\n",
    "    yref='paper', y=0.8,\n",
    "    xref='paper', x=0.04,\n",
    "    text=text,\n",
    "    font={'size':11},\n",
    "    showarrow=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Item Age <=1 RMSE',x=data.date[:-1],y=data.new_item_rmse[:-1]),\n",
    "    go.Bar(name='Item Age > 1 RMSE',x=data.date[:-1],y=data.old_item_rmse[:-1])\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='RMSE by Month for New and Old Items',\n",
    "    xaxis={'title':'Month','type':'category'},\n",
    "    yaxis={'title':'Mean Value'},\n",
    "    legend={'yanchor':'top','y':1.05,'xanchor':'left','x':0.01},\n",
    "    template='plotly_dark'\n",
    ")\n",
    "text = '''\n",
    "    For older items, performance on the validation set is no worse than <br>\n",
    "    performance on the training set. The higher overall validation set <br>\n",
    "    RMSE is caused by a significantly higher RMSE for new items.\n",
    "'''\n",
    "fig.add_annotation(yref='paper', y=1.1,\n",
    "                   xref='paper', x=0.93,\n",
    "                   text=text,\n",
    "                   font={'size':11},\n",
    "                   showarrow=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Table Views** <a id=\"42\"></a>\n",
    "\n",
    "There are 42 shops in our dataset. The following view shows us which shops have previously sold any items from each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../input/files-top-scoring-notebook-output-exploration/df_complete.pkl')\n",
    "(\n",
    "df\n",
    "[df['category_id'].isin(X_test.category_id.unique())]\n",
    ".query('item_cnt>0')\n",
    ".groupby('category_id')\n",
    ".agg({\n",
    "    'category_age':'max',\n",
    "    'shop_id':['nunique','unique'],\n",
    "    'item_cnt':'sum'\n",
    "    })\n",
    ".join(categories['category_name'])\n",
    ".join(\n",
    "    X_test       \n",
    "    .groupby('category_id')\n",
    "    .agg({'item_id':'nunique'})\n",
    "    .rename(columns={'item_id':'test_set_items'})\n",
    ")\n",
    ".sort_values(('shop_id', 'nunique'))\n",
    ".head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following view shows which shops are causing the most loss in our validation set. Shops 25, 31, 42, and 28 have the highest MSEs but also the highest target means (we would expect to see MSE scale with target mean). Shops 12 and 55 have the next highest MSEs, and do not have relatively high target means. These are the only two shops which have a category only they sell. Could they benefit from being segregated in the training stage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "X_val\n",
    "[X_val['category_id'].isin(X_test.category_id.unique())]\n",
    ".groupby('shop_id')\n",
    ".agg({\n",
    "    'sq_err':'mean',\n",
    "    'target':'mean',\n",
    "    'lgb_pred':'mean'\n",
    "})\n",
    ".join(\n",
    "    X_test\n",
    "    .rename(columns={'lgb_pred':'test_pred'})\n",
    "    .groupby('shop_id')\n",
    "    .agg({'test_pred':'mean'})\n",
    ")\n",
    ".join(shops['shop_name'])\n",
    ".sort_values('sq_err', ascending=False)\n",
    ".head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following view shows which categories are causing the most loss in our validation set, and how prominent these categories are within the test set. Looking at their MSE and presence in the test set shows us that the video game categories are the causing the most loss and might warrant especially close examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "X_val\n",
    "[X_val['category_id'].isin(X_test.category_id.unique())]\n",
    ".groupby('category_id')\n",
    ".agg({\n",
    "    'sq_err':['sum','mean'],\n",
    "    'target':'mean',\n",
    "    'lgb_pred':['sum','mean'],\n",
    "    'item_id':'nunique'\n",
    "})\n",
    ".join(\n",
    "    X_test\n",
    "    .rename(columns={'lgb_pred':'test_pred','item_id':'test_items'})\n",
    "    .groupby('category_id')        \n",
    "    .agg({\n",
    "        'test_pred':['sum','mean'],\n",
    "        'test_items':'nunique'\n",
    "    }),\n",
    "    on='category_id'\n",
    ")\n",
    ".join(categories)\n",
    ".sort_values(('sq_err', 'mean'), ascending=False)\n",
    ".head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following view shows which items have the least accurate predictions across all shops in the validation set. Changing the value of the CATEGORY variable will allow you to look through different categories of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit this value to look through the biggest problem items in each category\n",
    "CATEGORY = 20\n",
    "(\n",
    "items[\n",
    "    items['item_id'].isin(X_val.item_id.unique()) & \n",
    "    items['item_id'].isin(X_test.item_id.unique())\n",
    "]\n",
    "[['category_id','category_name','item_id','item_name']]\n",
    ".join(\n",
    "    X_test\n",
    "    .rename(columns={'lgb_pred':'test_pred'})\n",
    "    .groupby('item_id')        \n",
    "    .agg({'test_pred':'mean'}),\n",
    "    on='item_id'\n",
    ")\n",
    ".join(\n",
    "    X_val\n",
    "    .groupby('item_id')\n",
    "    .agg({\n",
    "        'lgb_pred':'mean',\n",
    "        'target':'mean',\n",
    "        'sq_err':'mean',\n",
    "        'same_name4catage_cnt_all_shops':'first',\n",
    "        'new_items_in_cat_all_shops_lag1to12':'first',\n",
    "        'item_cnt_all_shops_lag1':'first',\n",
    "        'category_cnt_all_shops_lag1':'first',\n",
    "        'item_cnt_sum_alltime_allshops':'first',\n",
    "        'prev_days_on_sale':'first'\n",
    "    })\n",
    "    .rename(columns={\n",
    "        'lgb_pred':'val_pred',\n",
    "        'target':'val_target',  \n",
    "    }),\n",
    "    on='item_id'\n",
    ")\n",
    ".query(f'category_id=={CATEGORY}')\n",
    ".sort_values('sq_err',ascending=False)\n",
    ".rename(columns={\n",
    "    'same_name4catage_cnt_all_shops':'name4mean',\n",
    "    'new_items_in_cat_all_shops_lag1to12':'new_in_cat_mean',\n",
    "    'item_cnt_all_shops_lag1':'item_cnt_lag1',\n",
    "    'category_cnt_all_shops_lag1':'cat_cnt_lag1',\n",
    "    'category_id':'cat',\n",
    "    'item_cnt_sum_alltime_allshops':'item_cnt_alltime'\n",
    "})\n",
    ".head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following view shows which items in each category have the highest prediction value across all shops in the test set. Again the CATEGORY variable can be changed to look through different categories of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = 20  \n",
    "(\n",
    "items[items['item_id'].isin(X_test.item_id.unique())]\n",
    "[['category_id','category_name','item_id','item_name']]\n",
    ".join(\n",
    "    X_test\n",
    "    .groupby('item_id')\n",
    "    .agg({\n",
    "        'lgb_pred':'mean',\n",
    "        'same_name4catage_cnt_all_shops':'first',\n",
    "        'new_items_in_cat_all_shops_lag1to12':'first',\n",
    "        'item_cnt_all_shops_lag1':'first',\n",
    "        'category_cnt_all_shops_lag1':'first',\n",
    "        'item_cnt_sum_alltime_allshops':'first',\n",
    "        'prev_days_on_sale':'first'\n",
    "    })\n",
    "    .rename(columns={'lgb_pred':'test_pred'}),\n",
    "    on='item_id'\n",
    ")\n",
    ".query(f'category_id=={CATEGORY}')\n",
    ".sort_values('test_pred',ascending=False)\n",
    ".rename(columns={\n",
    "     'same_name4catage_cnt_all_shops':'name4mean',\n",
    "     'new_items_in_cat_all_shops_lag1to12':'new_in_cat_mean',\n",
    "     'item_cnt_all_shops_lag1':'item_cnt_lag1',\n",
    "     'category_cnt_all_shops_lag1':'cat_cnt_lag1',\n",
    "     'item_category_id':'cat',\n",
    "     'item_cnt_sum_alltime_allshops':'item_cnt_alltime'\n",
    "})\n",
    ".head(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these final two views in tandem can help identify specific problematic items in the test set. It is easier to keep track of the information when all the columns can be seen at once in the wider view available when running the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
